<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index,follow" />
  <link rel="canonical" href="https://www.matomembowene.co.za/blog/rag-guardrails.html" />
  <title>RAG guardrails: being useful without hallucinating | Matome Mbowene</title>
  <meta name="description" content="A good RAG system is as much about knowing when to refuse as it is about retrieval. Notes on deterministic indexing, conservative thresholds, and provenance." />
  <meta property="og:title" content="RAG guardrails | Matome Mbowene" />
  <meta property="og:description" content="Building RAG systems that are helpful while staying honest and grounded." />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="https://www.matomembowene.co.za/blog/rag-guardrails.html" />
  <meta name="twitter:card" content="summary" />
  <meta name="theme-color" content="#0b0f14" />
  <style>
    :root {
      --bg: #0b0f14; --card: #12141a; --accent: #22c55e; --accent-2: #16a34a;
      --text: #f8fafc; --muted: #a1aab8; --border: rgba(148,163,184,0.16);
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Arial, sans-serif;
      background: var(--bg); color: var(--text);
      line-height: 1.8;
    }
    .container { width: min(720px, 100%); margin: 0 auto; padding: 2rem 1.5rem 4rem; }
    .topbar {
      display: flex; align-items: center; justify-content: space-between;
      gap: 1rem; flex-wrap: wrap; margin-bottom: 2rem;
      padding-bottom: 1rem; border-bottom: 1px solid var(--border);
    }
    .pill {
      display: inline-flex; align-items: center; gap: 0.45rem;
      padding: 0.55rem 0.85rem; border-radius: 999px;
      border: 1px solid rgba(148,163,184,0.20); background: rgba(148,163,184,0.08);
      color: var(--muted); text-decoration: none; font-weight: 750;
    }
    .pill:hover { border-color: rgba(34,197,94,0.35); background: rgba(34,197,94,0.10); }
    h1 { font-size: 1.8rem; letter-spacing: -0.02em; margin: 0 0 0.5rem; line-height: 1.3; }
    h2 { font-size: 1.25rem; margin: 2rem 0 0.75rem; color: var(--text); }
    h3 { font-size: 1.1rem; margin: 1.5rem 0 0.5rem; color: var(--text); }
    p, li { color: var(--muted); line-height: 1.8; }
    a { color: var(--accent); text-decoration: underline; text-underline-offset: 3px; }
    a:hover { color: var(--text); }
    .meta { color: var(--muted); font-size: 0.95rem; margin-bottom: 2rem; }
    code {
      background: rgba(148,163,184,0.12); padding: 0.15rem 0.4rem;
      border-radius: 4px; font-size: 0.9em; color: var(--text);
    }
    pre {
      background: rgba(148,163,184,0.08); border: 1px solid var(--border);
      border-radius: 8px; padding: 1rem 1.25rem; overflow-x: auto;
      font-size: 0.9rem; line-height: 1.6;
    }
    pre code { background: none; padding: 0; }
    blockquote {
      margin: 1.5rem 0; padding: 1rem 1.25rem;
      border-left: 3px solid var(--accent); background: rgba(34,197,94,0.06);
      border-radius: 0 8px 8px 0;
    }
    blockquote p { color: var(--text); margin: 0; }
    .tag {
      display: inline-block; border: 1px solid rgba(148,163,184,0.22);
      background: rgba(148,163,184,0.08); color: var(--muted);
      padding: 0.2rem 0.5rem; border-radius: 999px; font-size: 0.85rem; font-weight: 700;
    }
    .cta-card {
      background: rgba(34,197,94,0.06); border: 1px solid rgba(34,197,94,0.2);
      border-radius: 12px; padding: 1.5rem; margin-top: 2.5rem; text-align: center;
    }
    .cta-card p { color: var(--text); margin: 0 0 1rem; }
    footer { text-align: center; padding: 2rem 1rem; color: var(--muted); font-size: 0.9rem; border-top: 1px solid var(--border); }
    @media print {
      body { background: #fff; color: #000; }
      p, li { color: #222; }
      a { color: #000; }
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="topbar">
      <a class="pill" href="../">← Portfolio</a>
      <a class="pill" href="../#writing">All writing</a>
    </div>

    <article>
      <h1>RAG guardrails: being useful without hallucinating</h1>
      <div class="meta">
        <span>Matome Mbowene</span> · <span>2026</span> ·
        <span class="tag">RAG</span> <span class="tag">LLMs</span> <span class="tag">Reliability</span>
      </div>

      <p>
        Retrieval-augmented generation sounds straightforward in concept: retrieve relevant documents,
        feed them to a language model, get a grounded answer. In practice, the gap between "demo-ready"
        and "production-ready" is where most RAG systems fall apart.
      </p>

      <blockquote>
        <p>A good RAG system is as much about "when to refuse" as it is about retrieval.</p>
      </blockquote>

      <h2>The hallucination problem isn't just about the LLM</h2>
      <p>
        It's tempting to blame hallucinations on the language model alone, but in a RAG pipeline,
        the retrieval step introduces its own failure modes:
      </p>
      <ul>
        <li><strong>False relevance:</strong> A document that matches keywords but answers a different question.</li>
        <li><strong>Partial context:</strong> Chunks that contain related information but miss critical qualifiers.</li>
        <li><strong>Stale data:</strong> Documents that were accurate when indexed but are now outdated.</li>
        <li><strong>Missing evidence:</strong> The answer genuinely isn't in the corpus, but the model generates one anyway.</li>
      </ul>
      <p>
        Each of these produces a confident-sounding answer that's wrong in ways the user can't easily verify.
      </p>

      <h2>Principles I follow</h2>

      <h3>1. Deterministic indexing</h3>
      <p>
        The index should be reproducible. Same documents in, same index out. This means pinning
        embedding model versions, using deterministic chunking strategies, and versioning the
        index alongside the source documents. If you can't reproduce the index, you can't
        debug retrieval failures.
      </p>

      <h3>2. Conservative thresholds</h3>
      <p>
        Not every retrieved document is worth using. I set similarity thresholds that err on the
        side of returning "I don't have enough evidence to answer that" rather than surfacing
        weakly related content. The threshold is tuned per use case—a knowledge base query
        needs higher confidence than a discovery/browsing query.
      </p>

      <h3>3. Provenance by default</h3>
      <p>
        Every answer should link back to its source. This means including document references,
        chunk identifiers, and similarity scores in the response metadata. Users (and developers)
        need to be able to verify <em>where</em> an answer came from, not just what it says.
      </p>

      <h3>4. Explicit refusal</h3>
      <p>
        If retrieved evidence is below the confidence threshold, the system says so.
        "I don't have enough information to answer that" is a better response than a
        plausible-sounding guess. This is especially important in domains where wrong answers
        have real consequences.
      </p>

      <h3>5. Guardrails on generation</h3>
      <p>
        Even with good retrieval, the LLM can still drift. Guardrails include:
      </p>
      <ul>
        <li>Constraining the model to answer only from provided context.</li>
        <li>Detecting when the model's response contradicts or extends beyond the retrieved evidence.</li>
        <li>Rate limiting and input validation to prevent prompt injection.</li>
      </ul>

      <h2>What this looks like in practice</h2>
      <p>
        In the <a href="https://github.com/MatomeMb/personal-codex-agent">personal-codex-agent</a> project,
        the pipeline follows: ingestion → chunking → embeddings → persistent index → retrieval → response generation.
        The emphasis is on reproducibility at every step and conservative behavior when evidence is weak.
      </p>
      <p>
        The portfolio chatbot on this site follows similar principles at a smaller scale—it answers only
        from a local knowledge base and refuses sensitive or out-of-scope queries outright.
      </p>

      <h2>What I'd improve next</h2>
      <ul>
        <li>Add evaluation datasets for retrieval quality (relevance, recall, answer faithfulness).</li>
        <li>Implement re-ranking to improve retrieval precision beyond raw similarity.</li>
        <li>Build monitoring for retrieval distribution drift over time.</li>
        <li>Experiment with hybrid retrieval (keyword + semantic) for better coverage.</li>
      </ul>

      <h2>Takeaway</h2>
      <p>
        The bar for a useful RAG system isn't "can it generate answers?"—it's "can you trust
        the answers it gives, and can you tell when it doesn't know?"
        Deterministic indexing, conservative thresholds, provenance, and explicit refusal
        are the foundations that make the difference.
      </p>

      <div class="cta-card">
        <p>Want to discuss RAG architecture or retrieval reliability?</p>
        <a class="pill" href="mailto:matomepontso@gmail.com?subject=RAG%20discussion">Get in touch</a>
        <a class="pill" href="../case-studies/rag-assistant.html" style="margin-left: 0.5rem;">View case study</a>
      </div>
    </article>
  </div>

  <footer>
    <p>&copy; 2026 Matome Mbowene · <a href="../">Portfolio</a></p>
  </footer>
</body>
</html>
